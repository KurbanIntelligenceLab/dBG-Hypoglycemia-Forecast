{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Load and parse the Dataset (Run this first!)\n",
    "\n",
    "Update the directory which contains all xlsx files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ParseData import parse_dataset\n",
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "\n",
    "patient_data = parse_dataset(\"/home/meocakir/Documents/Datasets/Diabetes\", silent=False)\n",
    "patients = patient_data[Cols.patient].unique()\n",
    "\n",
    "len(patient_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter prune benchmark Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "from Benchmark import benchmark\n",
    "\n",
    "params = {\n",
    "    \"k\": 5,\n",
    "    \"risky_chars\": {0, 1},\n",
    "    \"risk_threshold\": 0.2,\n",
    "    \"prune\": True,\n",
    "    \"prune_method\": Opts.filter,\n",
    "    \"prune_threshold\": 1,\n",
    "    \"weight_thresholds\": [1, 3, 8],\n",
    "    \"value_ranges\": [(0, 2), (2, 3), (3, float('inf'))],\n",
    "    \"max_steps\": 6,\n",
    "    \"naive_threshold\": 15\n",
    "}\n",
    "\n",
    "excluded = patients.copy().tolist()\n",
    "\n",
    "excluded.remove('P17')\n",
    "excluded.remove('P26')\n",
    "\n",
    "benchmark(patient_data[patient_data[Cols.patient].isin(excluded)].copy(), start_time_range_hours=0,\n",
    "          end_time_range_hours=1, **params)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adaptive prune benchmark Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "from Benchmark import benchmark\n",
    "\n",
    "params = {\n",
    "    \"k\": 6,\n",
    "    \"risky_chars\": {0, 1},\n",
    "    \"risk_threshold\": 0.2,\n",
    "    \"prune\": False,\n",
    "    \"prune_method\": Opts.adaptive,\n",
    "    \"prune_threshold\": 1,\n",
    "    \"weight_thresholds\": [1, 4, 10],\n",
    "    \"value_ranges\": [(0, 2), (2, 3), (3, float('inf'))],\n",
    "    \"max_steps\": 6,\n",
    "    \"naive_threshold\": 15\n",
    "}\n",
    "excluded = patients.copy().tolist()\n",
    "\n",
    "#excluded.remove('P11')\n",
    "#excluded.remove('P26')\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "benchmark(patient_data[patient_data[Cols.patient].isin(excluded)].copy(), start_time_range_hours=0,\n",
    "          end_time_range_hours=1, **params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adaptive prune benchmark Test on 2 patients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Probability Distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.VisualizationUtils import draw_histogram\n",
    "from deBruijn.ProbabilityGraph import ProbabilityGraph\n",
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "\n",
    "k = 4\n",
    "risky_chars: None\n",
    "params = {\n",
    "    \"prune\": False,\n",
    "    \"prune_method\": Opts.filter,\n",
    "    \"prune_threshold\": 3,\n",
    "    \"max_steps\": 3,\n",
    "}\n",
    "\n",
    "sequences = []\n",
    "for p in patients:\n",
    "    float_seq = patient_data[patient_data[Cols.patient] == p]\n",
    "    float_seq = float_seq.sort_values(Cols.date, ascending=True)[Cols.char]\n",
    "    sequences.append(float_seq)\n",
    "\n",
    "probability_graph = ProbabilityGraph(sequences=sequences, k=k)\n",
    "\n",
    "print(f\"Resulting graph: {probability_graph}\")\n",
    "\n",
    "probability_model = probability_graph.get_probability_model(**params)\n",
    "\n",
    "draw_histogram(list(probability_model.probability_dict.values()), \"Node Probability Distribution\", \"Probability\",\n",
    "               \"Count\", bins=20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot Timeline of the target (ideal model) for every point. Alerted datapoints are marked as red"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.VisualizationUtils import draw_timeline\n",
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "\n",
    "from Benchmark import add_target_column\n",
    "\n",
    "naive_threshold = 20\n",
    "params = {\n",
    "    \"k\": 4,\n",
    "    \"risky_chars\": None,\n",
    "    \"risk_threshold\": 0.5,\n",
    "    \"prune\": True,\n",
    "    \"prune_method\": Opts.filter,\n",
    "    \"prune_threshold\": 3,\n",
    "    \"max_steps\": 3,\n",
    "}\n",
    "\n",
    "# Pick an alert model here\n",
    "alert_to_plot = Cols.target\n",
    "\n",
    "patient_data_with_alerts = add_target_column(patient_data)\n",
    "\n",
    "for p in ['P1']:\n",
    "    draw_timeline(\n",
    "        patient_data_with_alerts[patient_data_with_alerts[Cols.patient] == p].sort_values(Cols.date, ascending=True), p,\n",
    "        alert_to_plot, include_already_dangerous=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Draw timeline of one of our models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.VisualizationUtils import draw_timeline\n",
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "\n",
    "from Benchmark import add_alerts, add_target_column\n",
    "\n",
    "naive_threshold = 15\n",
    "\n",
    "params = {\n",
    "    \"k\": 6,\n",
    "    \"risky_chars\": {0, 1},\n",
    "    \"risk_threshold\": 0.2,\n",
    "    \"prune\": True,\n",
    "    \"prune_method\": Opts.adaptive,\n",
    "    \"prune_threshold\": 1,\n",
    "    \"weight_thresholds\": [1, 4, 10],\n",
    "    \"value_ranges\": [(0, 2), (2, 3), (3, float('inf'))],\n",
    "    \"max_steps\": 6,\n",
    "}\n",
    "\n",
    "# Pick an alert model here\n",
    "alert_to_plot = Cols.combined_alert_and\n",
    "\n",
    "patient_data_with_alerts = add_target_column(patient_data)\n",
    "patient_data_with_alerts = add_alerts(patient_data_with_alerts, naive_threshold, **params)\n",
    "\n",
    "excluded = patients.copy().tolist()\n",
    "\n",
    "for p in ['P1']:\n",
    "    draw_timeline(\n",
    "        patient_data_with_alerts[patient_data_with_alerts[Cols.patient] == p].sort_values(Cols.date, ascending=True), p,\n",
    "        Cols.prob_alert, include_already_dangerous=False)\n",
    "    draw_timeline(\n",
    "        patient_data_with_alerts[patient_data_with_alerts[Cols.patient] == p].sort_values(Cols.date, ascending=True), p,\n",
    "        Cols.combined_alert_and, include_already_dangerous=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "\n",
    "from Benchmark import add_alerts, add_target_column\n",
    "\n",
    "naive_threshold = 15\n",
    "\n",
    "params = {\n",
    "    \"k\": 6,\n",
    "    \"risky_chars\": {0, 1},\n",
    "    \"risk_threshold\": 0.2,\n",
    "    \"prune\": True,\n",
    "    \"prune_method\": Opts.adaptive,\n",
    "    \"prune_threshold\": 1,\n",
    "    \"weight_thresholds\": [1, 4, 10],\n",
    "    \"value_ranges\": [(0, 2), (2, 3), (3, float('inf'))],\n",
    "    \"max_steps\": 6,\n",
    "}\n",
    "\n",
    "patient_data_with_alerts = add_target_column(patient_data)\n",
    "patient_data_with_alerts = add_alerts(patient_data_with_alerts, naive_threshold, **params)\n",
    "\n",
    "# excluded.remove('P17')\n",
    "# excluded.remove('P26')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Benchmark import calculate_metrics\n",
    "\n",
    "metrics = list()\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "for p in patients:\n",
    "    data_patient = patient_data_with_alerts[patient_data_with_alerts[Cols.patient] == p].copy()\n",
    "    metric = calculate_metrics(data_patient, Cols.combined_alert_and)\n",
    "    metric['Patient'] = p\n",
    "    del metric['Accuracy']\n",
    "    print(p)\n",
    "    display(metric['Confusion Matrix'])\n",
    "    del metric['Confusion Matrix']\n",
    "    values_under_70 = data_patient[data_patient['Value'] < 70]\n",
    "    percentage_under_70 = (len(values_under_70) / len(data_patient)) * 100\n",
    "    metric['% Hypo'] = percentage_under_70\n",
    "    metrics.append(metric)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df_sorted = df.sort_values(by='Balanced Accuracy', ascending=False)\n",
    "display(df_sorted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "no_warning = 0\n",
    "gave_warning = 0\n",
    "\n",
    "for p in patients:\n",
    "    df = patient_data[patient_data[Cols.patient] == p].copy()\n",
    "    df = df.dropna(\n",
    "        subset=[Cols.target, Cols.naive_alert, Cols.prob_alert, Cols.combined_alert_and, Cols.combined_alert_or])\n",
    "    crossed_70 = (df[Cols.value] < 70) & (df[Cols.value].shift(1) >= 70)\n",
    "    alert_true = df[Cols.combined_alert_and].shift(1).astype(bool)\n",
    "    target_true = df[Cols.target].shift(1).astype(bool)\n",
    "\n",
    "    gave_warning += np.sum(crossed_70 & alert_true & target_true)\n",
    "    no_warning += np.sum(crossed_70 & ~alert_true & target_true)\n",
    "\n",
    "print('gave_warning:', gave_warning)\n",
    "print('no_warning:', no_warning)\n",
    "print(gave_warning / (gave_warning + no_warning))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.VisualizationUtils import draw_histogram\n",
    "\n",
    "time_diff_list = []\n",
    "\n",
    "pdict = dict()\n",
    "\n",
    "for p in patients:\n",
    "    print(p)\n",
    "    df = patient_data[patient_data[Cols.patient] == p].copy().sort_values(Cols.date, ascending=True).reset_index(\n",
    "        drop=True)\n",
    "    df = df.dropna(subset=[Cols.target, Cols.naive_alert, Cols.prob_alert, Cols.combined_alert_and,\n",
    "                           Cols.combined_alert_or]).reset_index(drop=True)  # Reset index after dropna\n",
    "    crossed_70 = (df[Cols.value] < 70) & (df[Cols.value].shift(1) >= 70)\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        # Check if value crosses below 70\n",
    "        if crossed_70[i]:\n",
    "            start_time = df.loc[i, Cols.date]\n",
    "            # Iterate backwards to find the earliest point where both alert and target are true\n",
    "            for j in range(i - 2, -1, -1):\n",
    "                # Ignore point if value goes under 70 again\n",
    "                if df.loc[j, Cols.value] < 70:\n",
    "                    break\n",
    "                # Check if both target and alert are true\n",
    "                elif bool(df.loc[1, Cols.combined_alert_and]) is False or bool(df.loc[1, Cols.target]):\n",
    "                    end_time = df.loc[j, Cols.date]\n",
    "                    time_diff = start_time - end_time\n",
    "                    time_diff_list.append(time_diff)\n",
    "                    if p not in pdict.keys():\n",
    "                        pdict[p] = 1\n",
    "                    else:\n",
    "                        pdict[p] += 1\n",
    "                    break  # Found the required point, no need to check further\n",
    "\n",
    "# Convert list of timedelta objects to desired format (e.g., total seconds)\n",
    "time_diff_seconds = [td.total_seconds() for td in time_diff_list]\n",
    "time_diff_minutes = [td / 60 for td in time_diff_seconds]\n",
    "time_diff_minutes = [x for x in time_diff_minutes if x <= 60]\n",
    "\n",
    "print(len(pdict), pdict)\n",
    "print(time_diff_minutes)\n",
    "\"\"\"draw_histogram(time_diff_minutes, 'Forecast Time Distribution of the Model', 'Minutes', 'Count', bins=7\n",
    "               , color='#0000FF', edgecolor='black')\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "\n",
    "patient_data[Cols.patient].value_counts()\n",
    "\n",
    "print(len(patient_data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values_under_70 = patient_data[patient_data['Value'] < 70]\n",
    "percentage_under_70 = (len(values_under_70) / len(patient_data)) * 100\n",
    "print(percentage_under_70)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spectral Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "from utils.PropertyNames import MethodOptions as Opts\n",
    "from deBruijn.ProbabilityGraph import ProbabilityGraph\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "params = {\n",
    "    \"k\": 5,\n",
    "    \"risky_chars\": {0, 1},\n",
    "    \"risk_threshold\": 0.2,\n",
    "    \"prune\": True,\n",
    "    \"prune_method\": Opts.adaptive,\n",
    "    \"prune_threshold\": 1,\n",
    "    \"weight_thresholds\": [1, 4, 10],\n",
    "    \"value_ranges\": [(0, 2), (2, 3), (3, float('inf'))],\n",
    "    \"max_steps\": 6,\n",
    "    \"naive_threshold\": 15\n",
    "}\n",
    "\n",
    "csv_filename = \"edge_properties.csv\"\n",
    "\n",
    "for p in ['P21']:\n",
    "    patient_df = patient_data[patient_data[Cols.patient] == p].sort_values(Cols.date, ascending=True)\n",
    "    patient_sequences = []\n",
    "    sequence = []\n",
    "    for index, row in patient_df.iterrows():\n",
    "        date_gap = row[Cols.date_gap]\n",
    "        seq_char = row[Cols.char]\n",
    "        if pd.isna(date_gap) or (date_gap < pd.Timedelta(minutes=20)):\n",
    "            sequence.append(seq_char)\n",
    "        elif len(sequence) > params['k']:\n",
    "            patient_sequences.append(sequence)\n",
    "            sequence = []\n",
    "\n",
    "    probability_graph = ProbabilityGraph(k=params['k'], sequences=patient_sequences)\n",
    "    graph_copy = probability_graph.graph.copy()\n",
    "    print(graph_copy)\n",
    "    adj_mat = nx.to_numpy_array(graph_copy.to_undirected())\n",
    "    sc = SpectralClustering(2, affinity='precomputed', n_init=100)\n",
    "    sc.fit(adj_mat)\n",
    "    print('spectral clustering')\n",
    "    print(len(sc.labels_), sc.labels_)\n",
    "\n",
    "    # Assign cluster labels to nodes as properties in the original directed graph\n",
    "    nodelist = list(graph_copy.nodes())  # Ensure the node list is in the same order as used in adjacency matrix\n",
    "    for i, label in enumerate(sc.labels_):\n",
    "        graph_copy.nodes[nodelist[i]]['cluster'] = label\n",
    "\n",
    "    # Convert edge data to a Pandas DataFrame\n",
    "    edge_df = pd.DataFrame([(*e[:2], *e[2].values()) for e in graph_copy.edges(data=True)],\n",
    "                           columns=['Source', 'Target'] + list(list(graph_copy.edges(data=True))[0][2].keys()))\n",
    "\n",
    "    edge_df['weight'] = edge_df['weight'].astype(int)\n",
    "    # Export edge DataFrame to CSV\n",
    "    edge_df.to_csv('/home/lumpus/Documents/Classes/Indiana/deBruijn/Clustering/network_edges.csv', index=False)\n",
    "\n",
    "    # Create a list of dictionaries for node attributes\n",
    "    node_list = []\n",
    "    for node, attrs in graph_copy.nodes(data=True):\n",
    "        attr_dict = {'Id': node}\n",
    "        attr_dict.update(attrs)\n",
    "        node_list.append(attr_dict)\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    node_df = pd.DataFrame(node_list)\n",
    "\n",
    "    # Export node attributes to CSV\n",
    "    node_df.to_csv('/home/lumpus/Documents/Classes/Indiana/deBruijn/Clustering/network_nodes.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def draw_timeline(event_times, threshold=15):\n",
    "    counter = Counter(event_times)\n",
    "\n",
    "    # Sort events\n",
    "    sorted_events = sorted(counter.items())\n",
    "\n",
    "    # Vertical spacing between events at the same time\n",
    "    vertical_step = 0.2\n",
    "\n",
    "    # Create timeline plot\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='Hypoglycemia Event (t=0)')  # Line at t=0\n",
    "    first_x_plotted = False  # Flag to track whether the first 'x' marker has been plotted\n",
    "\n",
    "    for time, count in sorted_events:\n",
    "        mirror_time = -time  # Multiply by -1 to mirror the timeline\n",
    "        if count > threshold:\n",
    "            # If count exceeds threshold, plot a single special marker\n",
    "            plt.scatter(mirror_time, 0, marker='X', s=100, color='green')\n",
    "            plt.annotate(f'{count}\\nWarnings', (mirror_time, 0), textcoords=\"offset points\", xytext=(0,10), ha='center')  # Bold text\n",
    "        else:\n",
    "            # Calculate starting vertical offset for this time event to center it\n",
    "            vertical_offset = -(count - 1) * vertical_step / 2\n",
    "            for _ in range(count):\n",
    "                plt.scatter(mirror_time, vertical_offset, marker='x', s=50, color='blue', label='Warning Instance' if not first_x_plotted else \"\")\n",
    "                vertical_offset += vertical_step\n",
    "                first_x_plotted = True\n",
    "\n",
    "    plt.xlabel('Time (minutes)')\n",
    "    plt.title('Hypoglycemia Forecast Timeline Compilation')\n",
    "    plt.yticks([])  # Hide y-axis\n",
    "    plt.xticks(range(-35, 1, 1))  # Set x-ticks\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Test function with sample data\n",
    "event_times = [30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 21.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 15.0, 30.0, 30.0, 28.0, 30.0, 30.0, 30.0, 30.0, 30.0, 31.0, 30.0, 30.0, 26.0, 30.0, 14.0, 24.0, 30.0, 5.0, 32.0, 30.0, 24.0, 10.0, 23.0, 30.0, 15.0, 18.0, 11.0, 30.0, 28.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 16.0, 30.0, 21.0, 30.0, 30.0, 28.0, 16.0, 30.0, 30.0, 30.0, 30.0, 30.0, 22.0, 20.0, 30.0, 23.0, 30.0, 30.0, 16.0, 27.0, 20.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 23.0, 30.0, 30.0, 30.0, 31.0, 30.0, 30.0, 30.0, 30.0, 32.0, 30.0, 15.0, 30.0, 30.0, 30.0, 20.0, 30.0, 30.0, 30.0, 31.0, 23.0, 30.0, 30.0, 30.0, 30.0, 30.0, 15.0, 32.0, 31.0, 30.0, 30.0, 14.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 28.0, 30.0, 21.0, 22.0, 22.0, 15.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 11.0, 7.0, 17.0, 6.0, 15.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 21.0, 30.0, 30.0, 30.0, 31.0, 30.0, 30.0, 30.0, 20.0, 30.0, 31.0, 30.0, 30.0, 30.0, 30.0, 31.0, 32.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 31.0, 30.0, 30.0, 30.0, 30.0, 19.0, 30.0, 30.0, 28.0, 30.0, 30.0, 30.0, 30.0, 30.0, 15.0, 30.0, 15.0, 15.0, 30.0, 30.0, 29.0, 15.0, 7.0, 31.0, 30.0, 30.0, 30.0, 28.0, 33.0, 30.0, 30.0, 29.0, 29.0, 30.0, 26.0, 30.0, 28.0, 30.0, 28.0, 30.0, 31.0, 30.0, 23.0, 30.0, 28.0, 22.0, 18.0, 30.0, 30.0, 30.0, 30.0, 30.0, 29.0, 30.0, 30.0, 30.0, 30.0, 31.0, 30.0, 30.0, 25.0, 31.0, 30.0, 30.0, 30.0]\n",
    "# event_times.extend([0]*10)\n",
    "print(event_times)\n",
    "draw_timeline(event_times)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "with open('Data/REPLACE_BG.dat', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "print(len(test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from utils.PropertyNames import ColumnNames as Cols\n",
    "\n",
    "with open('Data/REPLACE_BG.dat', 'rb') as f:\n",
    "    replaceBg = pickle.load(f)\n",
    "print(len(replaceBg))\n",
    "\n",
    "patients = replaceBg[Cols.patient].unique()\n",
    "\n",
    "result = replaceBg.groupby(Cols.patient)[Cols.isDangerous].sum().reset_index()\n",
    "total_count = replaceBg.groupby(Cols.patient).size().reset_index(name='Datapoints')\n",
    "\n",
    "final_result = pd.merge(result, total_count, on=Cols.patient)\n",
    "print(final_result)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
